"""
Advanced Reinforcement Learning Algorithms
Implements A3C, PPO, and SAC for optimal trading

Author: AutoTrade Pro
Version: 4.1
"""

from dataclasses import dataclass, asdict
from typing import List, Dict, Any, Optional, Tuple
import numpy as np
from datetime import datetime
from collections import deque
import json
import threading

try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    import torch.optim as optim
    from torch.distributions import Categorical, Normal
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    print("⚠️ PyTorch not available. Advanced RL will use mock agents.")


@dataclass
class RLAction:
    """Reinforcement learning action"""
    action_type: str  # 'buy', 'sell', 'hold'
    action_size: str  # 'small', 'medium', 'large'
    percentage: float
    confidence: float
    expected_reward: float
    algorithm: str  # 'a3c', 'ppo', 'sac'
    policy_entropy: float = 0.0
    value_estimate: float = 0.0


@dataclass
class RLPerformance:
    """RL algorithm performance metrics"""
    algorithm: str
    total_episodes: int
    avg_reward: float
    avg_episode_length: float
    success_rate: float
    total_profit: float
    sharpe_ratio: float
    max_drawdown: float


# ============================================================================
# 1. A3C (Asynchronous Advantage Actor-Critic)
# ============================================================================

class A3CNetwork(nn.Module if TORCH_AVAILABLE else object):
    """
    A3C network architecture

    Features:
    - Shared feature extraction
    - Separate actor and critic heads
    - Entropy regularization
    - Asynchronous training
    """

    def __init__(self, state_dim: int = 15, action_dim: int = 7, hidden_dim: int = 128):
        if TORCH_AVAILABLE:
            super(A3CNetwork, self).__init__()

            # Shared layers
            self.shared_fc1 = nn.Linear(state_dim, hidden_dim)
            self.shared_fc2 = nn.Linear(hidden_dim, hidden_dim)

            # Actor head (policy)
            self.actor_fc = nn.Linear(hidden_dim, hidden_dim // 2)
            self.actor_out = nn.Linear(hidden_dim // 2, action_dim)

            # Critic head (value function)
            self.critic_fc = nn.Linear(hidden_dim, hidden_dim // 2)
            self.critic_out = nn.Linear(hidden_dim // 2, 1)

    def forward(self, state):
        """Forward pass"""
        if not TORCH_AVAILABLE:
            return None, None

        # Shared layers
        x = F.relu(self.shared_fc1(state))
        x = F.relu(self.shared_fc2(x))

        # Actor (policy)
        actor = F.relu(self.actor_fc(x))
        action_probs = F.softmax(self.actor_out(actor), dim=-1)

        # Critic (value)
        critic = F.relu(self.critic_fc(x))
        state_value = self.critic_out(critic)

        return action_probs, state_value


class A3CAgent:
    """
    A3C Agent - Asynchronous Advantage Actor-Critic

    Features:
    - Asynchronous training with multiple workers
    - Advantage estimation
    - Entropy regularization
    - N-step returns
    """

    def __init__(self, state_dim: int = 15, action_dim: int = 7,
                 lr: float = 0.001, gamma: float = 0.99, entropy_coef: float = 0.01):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.entropy_coef = entropy_coef

        if TORCH_AVAILABLE:
            self.network = A3CNetwork(state_dim, action_dim)
            self.optimizer = optim.Adam(self.network.parameters(), lr=lr)
        else:
            self.network = None

        self.episode_rewards = []
        self.episode_count = 0

    def select_action(self, state: np.ndarray) -> Tuple[int, float, float]:
        """
        Select action using policy network

        Returns:
            action_idx: Selected action index
            entropy: Policy entropy
            value: State value estimate
        """
        if not TORCH_AVAILABLE:
            # Mock action
            action_idx = np.random.randint(0, self.action_dim)
            return action_idx, 0.8, 1.5

        self.network.eval()
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            action_probs, value = self.network(state_tensor)

            # Sample action from probability distribution
            dist = Categorical(action_probs)
            action = dist.sample()

            # Calculate entropy
            entropy = dist.entropy()

            return action.item(), entropy.item(), value.item()

    def train_step(self, states: List[np.ndarray], actions: List[int],
                   rewards: List[float], next_states: List[np.ndarray],
                   dones: List[bool]) -> Dict[str, float]:
        """
        Train A3C network

        Args:
            states: List of states
            actions: List of actions
            rewards: List of rewards
            next_states: List of next states
            dones: List of done flags

        Returns:
            Training metrics
        """
        if not TORCH_AVAILABLE:
            return {'actor_loss': 0.01, 'critic_loss': 0.02, 'entropy': 0.8}

        self.network.train()

        # Convert to tensors
        states_tensor = torch.FloatTensor(np.array(states))
        actions_tensor = torch.LongTensor(actions)
        rewards_tensor = torch.FloatTensor(rewards)

        # Forward pass
        action_probs, values = self.network(states_tensor)
        values = values.squeeze()

        # Calculate advantages
        returns = []
        R = 0
        for r, done in zip(reversed(rewards), reversed(dones)):
            if done:
                R = 0
            R = r + self.gamma * R
            returns.insert(0, R)
        returns = torch.FloatTensor(returns)
        advantages = returns - values.detach()

        # Actor loss (policy gradient)
        dist = Categorical(action_probs)
        log_probs = dist.log_prob(actions_tensor)
        actor_loss = -(log_probs * advantages).mean()

        # Critic loss (value function)
        critic_loss = F.mse_loss(values, returns)

        # Entropy bonus (exploration)
        entropy = dist.entropy().mean()

        # Total loss
        loss = actor_loss + 0.5 * critic_loss - self.entropy_coef * entropy

        # Optimize
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.network.parameters(), 0.5)
        self.optimizer.step()

        return {
            'actor_loss': actor_loss.item(),
            'critic_loss': critic_loss.item(),
            'entropy': entropy.item(),
            'total_loss': loss.item()
        }

    def get_performance(self) -> Dict[str, Any]:
        """Get performance metrics"""
        return {
            'algorithm': 'A3C',
            'total_episodes': self.episode_count,
            'avg_reward': np.mean(self.episode_rewards) if self.episode_rewards else 0.0,
            'entropy_coef': self.entropy_coef
        }


# ============================================================================
# 2. PPO (Proximal Policy Optimization)
# ============================================================================

class PPONetwork(nn.Module if TORCH_AVAILABLE else object):
    """
    PPO network with continuous and discrete action support

    Features:
    - Actor-Critic architecture
    - Clipped surrogate objective
    - GAE (Generalized Advantage Estimation)
    """

    def __init__(self, state_dim: int = 15, action_dim: int = 7, hidden_dim: int = 128):
        if TORCH_AVAILABLE:
            super(PPONetwork, self).__init__()

            # Actor network
            self.actor = nn.Sequential(
                nn.Linear(state_dim, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, action_dim),
                nn.Softmax(dim=-1)
            )

            # Critic network
            self.critic = nn.Sequential(
                nn.Linear(state_dim, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, 1)
            )

    def forward(self, state):
        """Forward pass"""
        if not TORCH_AVAILABLE:
            return None, None
        return self.actor(state), self.critic(state)


class PPOAgent:
    """
    PPO Agent - Proximal Policy Optimization

    Features:
    - Clipped surrogate objective
    - Multiple epochs of minibatch updates
    - GAE for advantage estimation
    - Entropy regularization
    """

    def __init__(self, state_dim: int = 15, action_dim: int = 7,
                 lr: float = 0.0003, gamma: float = 0.99, gae_lambda: float = 0.95,
                 clip_epsilon: float = 0.2, epochs: int = 10, batch_size: int = 64):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.gae_lambda = gae_lambda
        self.clip_epsilon = clip_epsilon
        self.epochs = epochs
        self.batch_size = batch_size

        if TORCH_AVAILABLE:
            self.network = PPONetwork(state_dim, action_dim)
            self.optimizer = optim.Adam(self.network.parameters(), lr=lr)
        else:
            self.network = None

        self.memory = {
            'states': [],
            'actions': [],
            'rewards': [],
            'values': [],
            'log_probs': [],
            'dones': []
        }

        self.performance = {
            'total_updates': 0,
            'avg_policy_loss': 0.0,
            'avg_value_loss': 0.0
        }

    def select_action(self, state: np.ndarray) -> Tuple[int, float, float, float]:
        """
        Select action using PPO policy

        Returns:
            action: Selected action
            log_prob: Log probability of action
            value: State value estimate
            entropy: Policy entropy
        """
        if not TORCH_AVAILABLE:
            action = np.random.randint(0, self.action_dim)
            return action, -1.5, 2.0, 0.85

        self.network.eval()
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            action_probs, value = self.network(state_tensor)

            dist = Categorical(action_probs)
            action = dist.sample()
            log_prob = dist.log_prob(action)
            entropy = dist.entropy()

            return action.item(), log_prob.item(), value.item(), entropy.item()

    def store_transition(self, state: np.ndarray, action: int, reward: float,
                        value: float, log_prob: float, done: bool):
        """Store transition in memory"""
        self.memory['states'].append(state)
        self.memory['actions'].append(action)
        self.memory['rewards'].append(reward)
        self.memory['values'].append(value)
        self.memory['log_probs'].append(log_prob)
        self.memory['dones'].append(done)

    def compute_gae(self) -> Tuple[np.ndarray, np.ndarray]:
        """
        Compute Generalized Advantage Estimation

        Returns:
            advantages: GAE advantages
            returns: Discounted returns
        """
        rewards = np.array(self.memory['rewards'])
        values = np.array(self.memory['values'])
        dones = np.array(self.memory['dones'])

        advantages = np.zeros_like(rewards)
        last_gae = 0

        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                next_value = 0
            else:
                next_value = values[t + 1]

            delta = rewards[t] + self.gamma * next_value * (1 - dones[t]) - values[t]
            advantages[t] = last_gae = delta + self.gamma * self.gae_lambda * (1 - dones[t]) * last_gae

        returns = advantages + values

        return advantages, returns

    def update(self) -> Dict[str, float]:
        """
        Update PPO policy

        Returns:
            Training metrics
        """
        if not TORCH_AVAILABLE or len(self.memory['states']) == 0:
            return {'policy_loss': 0.01, 'value_loss': 0.02}

        # Compute advantages and returns
        advantages, returns = self.compute_gae()

        # Normalize advantages
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

        # Convert to tensors
        states = torch.FloatTensor(np.array(self.memory['states']))
        actions = torch.LongTensor(self.memory['actions'])
        old_log_probs = torch.FloatTensor(self.memory['log_probs'])
        returns = torch.FloatTensor(returns)
        advantages = torch.FloatTensor(advantages)

        # PPO update for multiple epochs
        total_policy_loss = 0
        total_value_loss = 0

        for epoch in range(self.epochs):
            # Mini-batch updates
            indices = np.arange(len(states))
            np.random.shuffle(indices)

            for start in range(0, len(states), self.batch_size):
                end = start + self.batch_size
                batch_indices = indices[start:end]

                batch_states = states[batch_indices]
                batch_actions = actions[batch_indices]
                batch_old_log_probs = old_log_probs[batch_indices]
                batch_returns = returns[batch_indices]
                batch_advantages = advantages[batch_indices]

                # Forward pass
                action_probs, values = self.network(batch_states)
                dist = Categorical(action_probs)

                # New log probs and entropy
                new_log_probs = dist.log_prob(batch_actions)
                entropy = dist.entropy().mean()

                # Ratio for PPO
                ratio = torch.exp(new_log_probs - batch_old_log_probs)

                # Clipped surrogate objective
                surr1 = ratio * batch_advantages
                surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * batch_advantages
                policy_loss = -torch.min(surr1, surr2).mean()

                # Value loss
                value_loss = F.mse_loss(values.squeeze(), batch_returns)

                # Total loss
                loss = policy_loss + 0.5 * value_loss - 0.01 * entropy

                # Optimize
                self.optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.network.parameters(), 0.5)
                self.optimizer.step()

                total_policy_loss += policy_loss.item()
                total_value_loss += value_loss.item()

        # Clear memory
        self.memory = {k: [] for k in self.memory.keys()}

        self.performance['total_updates'] += 1
        self.performance['avg_policy_loss'] = total_policy_loss / (self.epochs * len(states) / self.batch_size)
        self.performance['avg_value_loss'] = total_value_loss / (self.epochs * len(states) / self.batch_size)

        return {
            'policy_loss': self.performance['avg_policy_loss'],
            'value_loss': self.performance['avg_value_loss']
        }

    def get_performance(self) -> Dict[str, Any]:
        """Get performance metrics"""
        return {
            'algorithm': 'PPO',
            'total_updates': self.performance['total_updates'],
            'avg_policy_loss': self.performance['avg_policy_loss'],
            'avg_value_loss': self.performance['avg_value_loss'],
            'clip_epsilon': self.clip_epsilon
        }


# ============================================================================
# 3. SAC (Soft Actor-Critic)
# ============================================================================

class SACNetwork(nn.Module if TORCH_AVAILABLE else object):
    """
    SAC network for continuous control

    Features:
    - Twin Q-networks (mitigate overestimation)
    - Stochastic policy
    - Automatic entropy tuning
    - Maximum entropy RL
    """

    def __init__(self, state_dim: int = 15, action_dim: int = 7, hidden_dim: int = 256):
        if TORCH_AVAILABLE:
            super(SACNetwork, self).__init__()

            # Actor network (stochastic policy)
            self.actor = nn.Sequential(
                nn.Linear(state_dim, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, hidden_dim),
                nn.ReLU()
            )
            self.mean = nn.Linear(hidden_dim, action_dim)
            self.log_std = nn.Linear(hidden_dim, action_dim)

            # Twin Q-networks
            self.q1 = nn.Sequential(
                nn.Linear(state_dim + action_dim, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, 1)
            )

            self.q2 = nn.Sequential(
                nn.Linear(state_dim + action_dim, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, 1)
            )

    def forward(self, state):
        """Forward pass for actor"""
        if not TORCH_AVAILABLE:
            return None, None

        x = self.actor(state)
        mean = self.mean(x)
        log_std = self.log_std(x)
        log_std = torch.clamp(log_std, -20, 2)

        return mean, log_std


class SACAgent:
    """
    SAC Agent - Soft Actor-Critic

    Features:
    - Maximum entropy RL
    - Twin delayed Q-learning
    - Automatic entropy tuning
    - Off-policy learning
    """

    def __init__(self, state_dim: int = 15, action_dim: int = 7,
                 lr: float = 0.0003, gamma: float = 0.99, tau: float = 0.005,
                 alpha: float = 0.2, buffer_size: int = 100000):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.tau = tau
        self.alpha = alpha

        if TORCH_AVAILABLE:
            self.network = SACNetwork(state_dim, action_dim)
            self.target_network = SACNetwork(state_dim, action_dim)
            self.target_network.load_state_dict(self.network.state_dict())

            self.actor_optimizer = optim.Adam(self.network.actor.parameters(), lr=lr)
            self.q_optimizer = optim.Adam(
                list(self.network.q1.parameters()) + list(self.network.q2.parameters()),
                lr=lr
            )

            # Automatic entropy tuning
            self.target_entropy = -action_dim
            self.log_alpha = torch.zeros(1, requires_grad=True)
            self.alpha_optimizer = optim.Adam([self.log_alpha], lr=lr)
        else:
            self.network = None

        self.replay_buffer = deque(maxlen=buffer_size)
        self.performance = {
            'total_steps': 0,
            'q_loss': 0.0,
            'policy_loss': 0.0,
            'alpha': alpha
        }

    def select_action(self, state: np.ndarray, deterministic: bool = False) -> Tuple[np.ndarray, float]:
        """
        Select action using SAC policy

        Args:
            state: Current state
            deterministic: If True, use mean action

        Returns:
            action: Selected action
            entropy: Action entropy
        """
        if not TORCH_AVAILABLE:
            action = np.random.uniform(-1, 1, self.action_dim)
            return action, 0.9

        self.network.eval()
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            mean, log_std = self.network(state_tensor)

            if deterministic:
                action = torch.tanh(mean)
            else:
                std = log_std.exp()
                dist = Normal(mean, std)
                z = dist.rsample()
                action = torch.tanh(z)

            entropy = -log_std.sum(dim=-1).mean().item()

            return action.numpy()[0], entropy

    def train_step(self, batch_size: int = 256) -> Dict[str, float]:
        """
        Train SAC networks

        Returns:
            Training metrics
        """
        if not TORCH_AVAILABLE or len(self.replay_buffer) < batch_size:
            return {'q_loss': 0.01, 'policy_loss': 0.02, 'alpha': self.alpha}

        # Sample batch
        batch = [self.replay_buffer[i] for i in np.random.choice(len(self.replay_buffer), batch_size)]
        states, actions, rewards, next_states, dones = zip(*batch)

        states = torch.FloatTensor(np.array(states))
        actions = torch.FloatTensor(np.array(actions))
        rewards = torch.FloatTensor(rewards).unsqueeze(1)
        next_states = torch.FloatTensor(np.array(next_states))
        dones = torch.FloatTensor(dones).unsqueeze(1)

        # Update Q-functions
        with torch.no_grad():
            next_mean, next_log_std = self.network(next_states)
            next_std = next_log_std.exp()
            next_dist = Normal(next_mean, next_std)
            next_z = next_dist.rsample()
            next_actions = torch.tanh(next_z)
            next_log_probs = next_dist.log_prob(next_z) - torch.log(1 - next_actions.pow(2) + 1e-6)
            next_log_probs = next_log_probs.sum(dim=-1, keepdim=True)

            next_q1 = self.target_network.q1(torch.cat([next_states, next_actions], dim=-1))
            next_q2 = self.target_network.q2(torch.cat([next_states, next_actions], dim=-1))
            next_q = torch.min(next_q1, next_q2) - self.alpha * next_log_probs
            target_q = rewards + (1 - dones) * self.gamma * next_q

        current_q1 = self.network.q1(torch.cat([states, actions], dim=-1))
        current_q2 = self.network.q2(torch.cat([states, actions], dim=-1))
        q_loss = F.mse_loss(current_q1, target_q) + F.mse_loss(current_q2, target_q)

        self.q_optimizer.zero_grad()
        q_loss.backward()
        self.q_optimizer.step()

        # Update policy
        mean, log_std = self.network(states)
        std = log_std.exp()
        dist = Normal(mean, std)
        z = dist.rsample()
        actions_new = torch.tanh(z)
        log_probs = dist.log_prob(z) - torch.log(1 - actions_new.pow(2) + 1e-6)
        log_probs = log_probs.sum(dim=-1, keepdim=True)

        q1_new = self.network.q1(torch.cat([states, actions_new], dim=-1))
        q2_new = self.network.q2(torch.cat([states, actions_new], dim=-1))
        q_new = torch.min(q1_new, q2_new)

        policy_loss = (self.alpha * log_probs - q_new).mean()

        self.actor_optimizer.zero_grad()
        policy_loss.backward()
        self.actor_optimizer.step()

        # Update alpha
        alpha_loss = -(self.log_alpha * (log_probs + self.target_entropy).detach()).mean()
        self.alpha_optimizer.zero_grad()
        alpha_loss.backward()
        self.alpha_optimizer.step()
        self.alpha = self.log_alpha.exp().item()

        # Soft update target networks
        for target_param, param in zip(self.target_network.parameters(), self.network.parameters()):
            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)

        self.performance['total_steps'] += 1
        self.performance['q_loss'] = q_loss.item()
        self.performance['policy_loss'] = policy_loss.item()
        self.performance['alpha'] = self.alpha

        return {
            'q_loss': q_loss.item(),
            'policy_loss': policy_loss.item(),
            'alpha': self.alpha
        }

    def get_performance(self) -> Dict[str, Any]:
        """Get performance metrics"""
        return {
            'algorithm': 'SAC',
            'total_steps': self.performance['total_steps'],
            'q_loss': self.performance['q_loss'],
            'policy_loss': self.performance['policy_loss'],
            'alpha': self.performance['alpha'],
            'buffer_size': len(self.replay_buffer)
        }


# ============================================================================
# Advanced RL Manager
# ============================================================================

class AdvancedRLManager:
    """
    Manager for all advanced RL algorithms

    Features:
    - A3C for asynchronous learning
    - PPO for stable policy updates
    - SAC for continuous control
    - Algorithm selection based on market conditions
    """

    def __init__(self):
        self.a3c_agent = A3CAgent()
        self.ppo_agent = PPOAgent()
        self.sac_agent = SACAgent()

        self.current_algorithm = 'ppo'  # Default
        self.performance_history = []

    def select_algorithm(self, market_volatility: float, trend_strength: float) -> str:
        """
        Select best algorithm based on market conditions

        Args:
            market_volatility: Market volatility (0-1)
            trend_strength: Trend strength (0-1)

        Returns:
            Best algorithm name
        """
        # High volatility → SAC (better exploration)
        if market_volatility > 0.7:
            return 'sac'
        # Strong trend → PPO (stable exploitation)
        elif trend_strength > 0.6:
            return 'ppo'
        # Normal conditions → A3C (balanced)
        else:
            return 'a3c'

    def get_action(self, state: np.ndarray, algorithm: str = None) -> RLAction:
        """
        Get trading action from selected algorithm

        Args:
            state: Current market state
            algorithm: Algorithm to use (None = auto-select)

        Returns:
            Recommended action
        """
        if algorithm is None:
            algorithm = self.current_algorithm

        # Map continuous actions to discrete
        action_mapping = {
            0: ('hold', 'none', 0),
            1: ('buy', 'small', 10),
            2: ('buy', 'medium', 20),
            3: ('buy', 'large', 30),
            4: ('sell', 'small', 10),
            5: ('sell', 'medium', 30),
            6: ('sell', 'large', 50)
        }

        if algorithm == 'a3c':
            action_idx, entropy, value = self.a3c_agent.select_action(state)
            action_type, action_size, percentage = action_mapping[action_idx]
            confidence = 1.0 - entropy
            expected_reward = value

        elif algorithm == 'ppo':
            action_idx, log_prob, value, entropy = self.ppo_agent.select_action(state)
            action_type, action_size, percentage = action_mapping[action_idx]
            confidence = np.exp(log_prob)
            expected_reward = value

        elif algorithm == 'sac':
            action_continuous, entropy = self.sac_agent.select_action(state)
            # Convert continuous to discrete
            action_idx = int((action_continuous[0] + 1) / 2 * 7) % 7
            action_type, action_size, percentage = action_mapping[action_idx]
            confidence = 1.0 - min(entropy, 1.0)
            expected_reward = np.mean(action_continuous)

        else:
            # Default
            action_type, action_size, percentage = 'hold', 'none', 0
            confidence = 0.5
            expected_reward = 0.0
            entropy = 0.8

        return RLAction(
            action_type=action_type,
            action_size=action_size,
            percentage=percentage,
            confidence=float(confidence),
            expected_reward=float(expected_reward),
            algorithm=algorithm,
            policy_entropy=float(entropy) if isinstance(entropy, (int, float)) else 0.8,
            value_estimate=float(expected_reward)
        )

    def get_all_performances(self) -> Dict[str, Any]:
        """Get performance metrics for all algorithms"""
        return {
            'a3c': self.a3c_agent.get_performance(),
            'ppo': self.ppo_agent.get_performance(),
            'sac': self.sac_agent.get_performance(),
            'current_algorithm': self.current_algorithm
        }


# Singleton instance
_advanced_rl_manager = None

def get_advanced_rl_manager() -> AdvancedRLManager:
    """Get singleton instance of advanced RL manager"""
    global _advanced_rl_manager
    if _advanced_rl_manager is None:
        _advanced_rl_manager = AdvancedRLManager()
    return _advanced_rl_manager


if __name__ == '__main__':
    print("🎮 Advanced RL Test")
    print(f"PyTorch Available: {TORCH_AVAILABLE}")

    manager = get_advanced_rl_manager()

    # Mock test
    state = np.random.randn(15)
    action = manager.get_action(state, algorithm='ppo')

    print(f"\n행동 추천:")
    print(f"타입: {action.action_type}")
    print(f"크기: {action.action_size} ({action.percentage}%)")
    print(f"신뢰도: {action.confidence:.1%}")
    print(f"예상 리워드: {action.expected_reward:.2f}")
    print(f"알고리즘: {action.algorithm}")
    print(f"정책 엔트로피: {action.policy_entropy:.2f}")

    print(f"\n전체 성과:")
    for algo, perf in manager.get_all_performances().items():
        if algo != 'current_algorithm':
            print(f"{algo.upper()}: {perf}")
